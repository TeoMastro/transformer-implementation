{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "000fc1af-8dc3-407a-8fc6-6df3e2d2bbbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (4.36.2)\n",
      "Requirement already satisfied: datasets in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (2.16.1)\n",
      "Requirement already satisfied: evaluate in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (0.4.1)\n",
      "Requirement already satisfied: sacrebleu in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (2.4.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from transformers) (0.20.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from transformers) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from transformers) (0.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from datasets) (14.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from datasets) (2.1.4)\n",
      "Requirement already satisfied: xxhash in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from datasets) (3.9.1)\n",
      "Requirement already satisfied: responses<0.19 in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: portalocker in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from sacrebleu) (2.8.2)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from sacrebleu) (0.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from sacrebleu) (0.4.6)\n",
      "Requirement already satisfied: lxml in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from sacrebleu) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from requests->transformers) (2023.11.17)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from pandas->datasets) (2023.4)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from portalocker->sacrebleu) (305.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers datasets evaluate sacrebleu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637acde7-e550-40c2-81f4-b59c229a05e6",
   "metadata": {},
   "source": [
    "# Load a sample dataset and split to train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8fb2cb0e-82b3-4d99-9c98-289b27bf1b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "books = load_dataset(\"opus_books\", \"en-es\") # isws vrw kai allo ena dataset en-es\n",
    "books = books[\"train\"].train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be2ec24-6535-4885-a81b-6806c4ce11c9",
   "metadata": {},
   "source": [
    "# An example of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8f25df30-222d-4845-909c-64af46bb493a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '2030',\n",
       " 'translation': {'en': \"The tea things were brought in, and already had Marianne been disappointed more than once by a rap at a neighbouring door, when a loud one was suddenly heard which could not be mistaken for one at any other house, Elinor felt secure of its announcing Willoughby's approach, and Marianne, starting up, moved towards the door.\",\n",
       "  'es': 'Trajeron las cosas para el té, y ya Marianne había tenido más de una decepción ante los golpes en alguna puerta vecina, cuando de repente se escuchó uno muy fuerte que no podía confundirse con alguno en otra casa. Elinor se sintió segura de que anunciaba la llegada de Willoughby, y Marianne, levantándose de un salto, se dirigió hacia la puerta.'}}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f539bab-e3a9-402e-a997-9c960dcc5725",
   "metadata": {},
   "source": [
    "# Import a tokenizer to process language pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0534457a-c896-4539-be30-ae19a8303e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24d5c25-8843-48e9-af8f-31707fd61f05",
   "metadata": {},
   "source": [
    "# Preprocess language pairs by tokenizing inputs and targets separately\n",
    "(since you can’t tokenize Spanish text with a tokenizer pretrained on an English vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "34b3cb78-df06-488e-9e73-7e2479f1cb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_lang = \"en\"\n",
    "target_lang = \"es\"\n",
    "prefix = \"translate English to Spanish: \"\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + example[source_lang] for example in examples[\"translation\"]]\n",
    "    targets = [example[target_lang] for example in examples[\"translation\"]]\n",
    "    model_inputs = tokenizer(inputs, text_target=targets, max_length=128, truncation=True) # 128 for t5-small\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0786645f-64ab-4ca7-9f51-9ed03fe721d3",
   "metadata": {},
   "source": [
    "# Apply the preprocess function to the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e6a6e219-3735-411d-ab15-6e92d184368e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48d44add0cf741afa86ab4544adbea7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/74776 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0576ceea2a844b74b2416128025a8064",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/18694 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_books = books.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbbfe40-dc49-4c66-864a-021861e356a9",
   "metadata": {},
   "source": [
    "# Create batches of dict-like objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bc9b911a-9a9b-4c7d-8f52-9b1606ead286",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b760de15-af3d-46a3-850e-ef5cc4715b63",
   "metadata": {},
   "source": [
    "# Import a metric for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6908e6cd-2f1e-47fa-ab5c-00b5b65bfe26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "# more metrics\n",
    "metric = evaluate.load(\"sacrebleu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02ada2d-cbe7-40ce-adda-9543623025f1",
   "metadata": {},
   "source": [
    "# Create a function that passes your predictions and labels to compute to calculate the SacreBLEU score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8f68a06f-2175-4985-b816-84f60786d84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result = {\"bleu\": result[\"score\"]}\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c24fda8-9c7b-4de7-8c9d-81e71ec0a21e",
   "metadata": {},
   "source": [
    "# Transformer finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ea83d75f-49a1-4a6f-87dc-1b8e40213946",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamWeightDecay\n",
    "\n",
    "optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "412864b5-1420-4cc5-991c-94167bdb52b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForSeq2SeqLM\n",
    "\n",
    "model = TFAutoModelForSeq2SeqLM.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5619fb7b-f555-4c6b-a8ae-67167f081838",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Dataset argument should be a datasets.Dataset!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 11\u001b[0m\n\u001b[0;32m      2\u001b[0m half \u001b[38;5;241m=\u001b[39m total_rows \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m      4\u001b[0m tf_train_set \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mprepare_tf_dataset(\n\u001b[0;32m      5\u001b[0m     tokenized_books[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m      6\u001b[0m     shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m      7\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,\n\u001b[0;32m      8\u001b[0m     collate_fn\u001b[38;5;241m=\u001b[39mdata_collator,\n\u001b[0;32m      9\u001b[0m )\n\u001b[1;32m---> 11\u001b[0m tf_test_set \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_tf_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenized_books\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mhalf\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_collator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m tf_validation_set \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mprepare_tf_dataset(\n\u001b[0;32m     19\u001b[0m     tokenized_books[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m][half:],\n\u001b[0;32m     20\u001b[0m     shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     21\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,\n\u001b[0;32m     22\u001b[0m     collate_fn\u001b[38;5;241m=\u001b[39mdata_collator,\n\u001b[0;32m     23\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorgpu\\lib\\site-packages\\transformers\\modeling_tf_utils.py:1453\u001b[0m, in \u001b[0;36mTFPreTrainedModel.prepare_tf_dataset\u001b[1;34m(self, dataset, batch_size, shuffle, tokenizer, collate_fn, collate_fn_args, drop_remainder, prefetch)\u001b[0m\n\u001b[0;32m   1450\u001b[0m     collate_fn_args \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m   1452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dataset, datasets\u001b[38;5;241m.\u001b[39mDataset):\n\u001b[1;32m-> 1453\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset argument should be a datasets.Dataset!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1454\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall)\u001b[38;5;241m.\u001b[39mparameters)\n\u001b[0;32m   1455\u001b[0m model_labels \u001b[38;5;241m=\u001b[39m find_labels(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: Dataset argument should be a datasets.Dataset!"
     ]
    }
   ],
   "source": [
    "tf_train_set = model.prepare_tf_dataset(\n",
    "    tokenized_books[\"train\"],\n",
    "    shuffle=True,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "tf_test_set = model.prepare_tf_dataset(\n",
    "    tokenized_books[\"test\"],\n",
    "    shuffle=False,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f38741-0412-42f5-b145-4dced18f9fd2",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "002160e1-b31b-4019-b48e-4fa3973613c8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf_validation_set' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras_callbacks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KerasMetricCallback\n\u001b[1;32m----> 3\u001b[0m metric_callback \u001b[38;5;241m=\u001b[39m KerasMetricCallback(metric_fn\u001b[38;5;241m=\u001b[39mcompute_metrics, eval_dataset\u001b[38;5;241m=\u001b[39m\u001b[43mtf_validation_set\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf_validation_set' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers.keras_callbacks import KerasMetricCallback\n",
    "\n",
    "metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_validation_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60058168-f529-4e91-a06c-a3875b86f73b",
   "metadata": {},
   "source": [
    "# Compile and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88fbf51-34f7-4e14-9c27-ca44e9e03569",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)\n",
    "model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a92437c-b5cd-4da0-b27e-a2e39346c7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8053deb-b846-4c19-85c1-3d1b3d940f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4673/4673 [==============================] - 973s 205ms/step - loss: 3.1877 - val_loss: 2.6240\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=1, callbacks=[metric_callback])\n",
    "model.save_pretrained(\"tf_model/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e2a5cd-dc8f-4c79-bf0a-5aceca50a763",
   "metadata": {},
   "source": [
    "## Plot accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "efe736fa-b9dd-4f04-beee-c97935e14ef7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mhistory\u001b[49m\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel Accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "628aaf80-ad75-4ccb-ae57-f3cc248066da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the layers of TFT5ForConditionalGeneration were initialized from the model checkpoint at tf_model/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "input_text = \"translate English to Spanish: Legumes share resources with nitrogen-fixing bacteria.\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = TFAutoModelForSeq2SeqLM.from_pretrained(\"tf_model/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b96ec2-2fc7-40ed-8b1e-0b3cd4141dae",
   "metadata": {},
   "source": [
    "## Generate the translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "49572d72-508f-4a71-a21e-49e4bab23b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[    0   312  1744  2687     3     9  5444  4922     3    15    40     3\n",
      "     60     7    23  1259    32     3     9    50     7     3  9305     9\n",
      "      7    20  2210   291     3 17694    17     9     5     1]], shape=(1, 34), dtype=int32)\n",
      "Legumes asoció el residuo a las bacas de fixar azota.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TEO\\anaconda3\\envs\\tensorgpu\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3860: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenized = tokenizer([input_text], return_tensors='np')\n",
    "out = model.generate(**tokenized, max_length=128)\n",
    "print(out)\n",
    "\n",
    "with tokenizer.as_target_tokenizer():\n",
    "    print(tokenizer.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbf168e-b2b3-4a3d-ace9-3cb59ff95d70",
   "metadata": {},
   "source": [
    "# Second Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f9df47a-7428-4209-abfd-82de58cca01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "43fb8aa3-9f4a-4c86-8d62-110c7bc10f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"This is a sentence that we want to translate.\"\n",
    "\n",
    "tokenizer.src_lang = \"en_XX\"\n",
    "tokenizer.tgt_lang = \"es_XX\"\n",
    "encoded_input = tokenizer(input_text, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28553470-24a1-481b-8795-17f08abcffde",
   "metadata": {},
   "source": [
    "## Translate the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c763253a-842f-4012-96a1-16d8ce2ac324",
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_tokens = model.generate(**encoded_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57a7ed9-981b-4b70-8977-292911d4f822",
   "metadata": {},
   "source": [
    "## Tokens to strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d327fda8-5dbd-4919-b6bd-9534c431d348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Questa è una frase che vogliamo tradurre.\n"
     ]
    }
   ],
   "source": [
    "translated_text = tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n",
    "print(translated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781963d3-1e57-43ef-9571-fe73a927a9b3",
   "metadata": {},
   "source": [
    "# Third implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51661bf3-7575-4c57-8b3c-66e418229e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hola, ¿cómo estás?\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "model_name = \"Helsinki-NLP/opus-mt-en-es\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "# Example usage\n",
    "input_text = \"Hello, how are you?\"\n",
    "tokenized = tokenizer([input_text], return_tensors='pt', max_length=512, truncation=True, padding=\"max_length\")\n",
    "translated = model.generate(**tokenized)\n",
    "translated_text = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "print(translated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
