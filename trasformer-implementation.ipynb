{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "000fc1af-8dc3-407a-8fc6-6df3e2d2bbbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (4.36.2)\n",
      "Requirement already satisfied: datasets in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (2.16.1)\n",
      "Requirement already satisfied: evaluate in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (0.4.1)\n",
      "Requirement already satisfied: sacrebleu in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (2.4.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from transformers) (0.20.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from transformers) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from transformers) (0.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from datasets) (14.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from datasets) (2.1.4)\n",
      "Requirement already satisfied: xxhash in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from datasets) (3.9.1)\n",
      "Requirement already satisfied: responses<0.19 in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: portalocker in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from sacrebleu) (2.8.2)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from sacrebleu) (0.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from sacrebleu) (0.4.6)\n",
      "Requirement already satisfied: lxml in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from sacrebleu) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from requests->transformers) (2023.11.17)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from pandas->datasets) (2023.4)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from portalocker->sacrebleu) (305.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers datasets evaluate sacrebleu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637acde7-e550-40c2-81f4-b59c229a05e6",
   "metadata": {},
   "source": [
    "# Load a sample dataset and split to train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "8fb2cb0e-82b3-4d99-9c98-289b27bf1b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "books = load_dataset(\"opus_books\", \"en-es\") # isws vrw kai allo ena dataset en-es\n",
    "books = books[\"train\"].train_test_split(test_size=0.2)\n",
    "\n",
    "books_train_validation_split = books[\"train\"].train_test_split(test_size=0.1)\n",
    "\n",
    "books[\"train\"] = books_train_validation_split[\"train\"]\n",
    "books[\"validation\"] = books_train_validation_split[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be2ec24-6535-4885-a81b-6806c4ce11c9",
   "metadata": {},
   "source": [
    "# An example of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "8f25df30-222d-4845-909c-64af46bb493a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '80523',\n",
       " 'translation': {'en': 'During all this time the necessary works had not been neglected.',\n",
       "  'es': 'Durante aquel tiempo no se habían descuidado las obras necesarias.'}}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f539bab-e3a9-402e-a997-9c960dcc5725",
   "metadata": {},
   "source": [
    "# Import a tokenizer to process language pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "0534457a-c896-4539-be30-ae19a8303e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, model_max_length=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24d5c25-8843-48e9-af8f-31707fd61f05",
   "metadata": {},
   "source": [
    "# Preprocess language pairs by tokenizing inputs and targets separately\n",
    "(since you can’t tokenize Spanish text with a tokenizer pretrained on an English vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "34b3cb78-df06-488e-9e73-7e2479f1cb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_lang = \"en\"\n",
    "target_lang = \"es\"\n",
    "prefix = \"translate English to Spanish: \"\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + example[source_lang] for example in examples[\"translation\"]]\n",
    "    targets = [example[target_lang] for example in examples[\"translation\"]]\n",
    "    model_inputs = tokenizer(inputs, text_target=targets, max_length=128, truncation=True) # 128 for t5-small\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0786645f-64ab-4ca7-9f51-9ed03fe721d3",
   "metadata": {},
   "source": [
    "# Apply the preprocess function to the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e6a6e219-3735-411d-ab15-6e92d184368e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58ccf5c4c4624ad19be8e3ae9d128404",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/67298 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6182f461b1414c62b5d77f247cd32e82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/18694 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b53fe31aba14977a5f807620525a7e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7478 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_books = books.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbbfe40-dc49-4c66-864a-021861e356a9",
   "metadata": {},
   "source": [
    "# Create batches of dict-like objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "bc9b911a-9a9b-4c7d-8f52-9b1606ead286",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b760de15-af3d-46a3-850e-ef5cc4715b63",
   "metadata": {},
   "source": [
    "# Import a metric for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "6908e6cd-2f1e-47fa-ab5c-00b5b65bfe26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "# more metrics\n",
    "metric = evaluate.load(\"sacrebleu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02ada2d-cbe7-40ce-adda-9543623025f1",
   "metadata": {},
   "source": [
    "# Create a function that passes your predictions and labels to compute to calculate the SacreBLEU score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "8f68a06f-2175-4985-b816-84f60786d84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    try:\n",
    "        preds, labels = eval_preds\n",
    "        if isinstance(preds, tuple):\n",
    "            preds = preds[0]\n",
    "        decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    \n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "        decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "    \n",
    "        result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "        result = {\"bleu\": result[\"score\"]}\n",
    "    \n",
    "        prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "        result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "        result = {k: round(v, 4) for k, v in result.items()}\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(\"Error in compute_metrics:\", e)\n",
    "        # Optionally, re-raise the error after logging\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c24fda8-9c7b-4de7-8c9d-81e71ec0a21e",
   "metadata": {},
   "source": [
    "# Transformer finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ea83d75f-49a1-4a6f-87dc-1b8e40213946",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamWeightDecay\n",
    "\n",
    "optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "412864b5-1420-4cc5-991c-94167bdb52b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForSeq2SeqLM\n",
    "\n",
    "model = TFAutoModelForSeq2SeqLM.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "5619fb7b-f555-4c6b-a8ae-67167f081838",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "tf_train_set = model.prepare_tf_dataset(\n",
    "    tokenized_books[\"train\"],\n",
    "    shuffle=True,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "tf_test_set = model.prepare_tf_dataset(\n",
    "    tokenized_books[\"test\"],\n",
    "    shuffle=False,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "tf_validation_set = model.prepare_tf_dataset(\n",
    "    tokenized_books[\"validation\"],\n",
    "    shuffle=False,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "9b13227f-dd2e-4b2e-9050-f9b6fd10af6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'input_ids': <tf.Tensor: shape=(16, 119), dtype=int64, numpy=\n",
      "array([[13959,  1566,    12, ...,     0,     0,     0],\n",
      "       [13959,  1566,    12, ...,     0,     0,     0],\n",
      "       [13959,  1566,    12, ...,     0,     0,     0],\n",
      "       ...,\n",
      "       [13959,  1566,    12, ...,     0,     0,     0],\n",
      "       [13959,  1566,    12, ...,     0,     0,     0],\n",
      "       [13959,  1566,    12, ...,     0,     0,     0]], dtype=int64)>, 'attention_mask': <tf.Tensor: shape=(16, 119), dtype=int64, numpy=\n",
      "array([[1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0]], dtype=int64)>}, <tf.Tensor: shape=(16, 128), dtype=int64, numpy=\n",
      "array([[    3,     2,   476, ...,  -100,  -100,  -100],\n",
      "       [    3,     2, 15046, ...,  -100,  -100,  -100],\n",
      "       [  180,     2,     6, ...,  -100,  -100,  -100],\n",
      "       ...,\n",
      "       [ 1915,    32,   975, ...,  -100,  -100,  -100],\n",
      "       [  325,     3,    29, ...,  -100,  -100,  -100],\n",
      "       [ 1289,   399,    52, ...,  -100,  -100,  -100]], dtype=int64)>)\n",
      "({'input_ids': <tf.Tensor: shape=(16, 73), dtype=int64, numpy=\n",
      "array([[13959,  1566,    12, ...,     0,     0,     0],\n",
      "       [13959,  1566,    12, ...,     0,     0,     0],\n",
      "       [13959,  1566,    12, ...,     0,     0,     0],\n",
      "       ...,\n",
      "       [13959,  1566,    12, ...,    97,     5,     1],\n",
      "       [13959,  1566,    12, ...,     0,     0,     0],\n",
      "       [13959,  1566,    12, ...,     0,     0,     0]], dtype=int64)>, 'attention_mask': <tf.Tensor: shape=(16, 73), dtype=int64, numpy=\n",
      "array([[1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [1, 1, 1, ..., 1, 1, 1],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0]], dtype=int64)>}, <tf.Tensor: shape=(16, 120), dtype=int64, numpy=\n",
      "array([[ 4701,     3,    15, ...,  -100,  -100,  -100],\n",
      "       [    3,     2, 15046, ...,  -100,  -100,  -100],\n",
      "       [  679, 25377,     9, ...,  -100,  -100,  -100],\n",
      "       ...,\n",
      "       [    3,   104,   134, ...,    32,     5,     1],\n",
      "       [ 7983,     7,   235, ...,  -100,  -100,  -100],\n",
      "       [    3,     2, 12998, ...,  -100,  -100,  -100]], dtype=int64)>)\n",
      "({'input_ids': <tf.Tensor: shape=(16, 78), dtype=int64, numpy=\n",
      "array([[13959,  1566,    12, ...,     0,     0,     0],\n",
      "       [13959,  1566,    12, ...,     0,     0,     0],\n",
      "       [13959,  1566,    12, ...,     0,     0,     0],\n",
      "       ...,\n",
      "       [13959,  1566,    12, ...,     0,     0,     0],\n",
      "       [13959,  1566,    12, ...,     0,     0,     0],\n",
      "       [13959,  1566,    12, ...,     0,     0,     0]], dtype=int64)>, 'attention_mask': <tf.Tensor: shape=(16, 78), dtype=int64, numpy=\n",
      "array([[1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0]], dtype=int64)>}, <tf.Tensor: shape=(16, 126), dtype=int64, numpy=\n",
      "array([[2133, 1188,   15, ..., -100, -100, -100],\n",
      "       [   3,    2,  382, ..., -100, -100, -100],\n",
      "       [8188,   15,    9, ..., -100, -100, -100],\n",
      "       ...,\n",
      "       [   3,    2, 5991, ..., -100, -100, -100],\n",
      "       [1636,    2, 5991, ..., -100, -100, -100],\n",
      "       [3144, 3368,   15, ..., -100, -100, -100]], dtype=int64)>)\n"
     ]
    }
   ],
   "source": [
    "for batch in tf_train_set.take(1):\n",
    "    print(batch)\n",
    "for batch in tf_test_set.take(1):\n",
    "    print(batch)\n",
    "for batch in tf_validation_set.take(1):\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f38741-0412-42f5-b145-4dced18f9fd2",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "002160e1-b31b-4019-b48e-4fa3973613c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.keras_callbacks import KerasMetricCallback\n",
    "\n",
    "metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_validation_set, predict_with_generate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60058168-f529-4e91-a06c-a3875b86f73b",
   "metadata": {},
   "source": [
    "# Compile and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "b88fbf51-34f7-4e14-9c27-ca44e9e03569",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)\n",
    "model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccb9990-eebd-4fcc-8361-07fd71a57c88",
   "metadata": {},
   "source": [
    "## See the model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "bc2bf6b7-a208-41fb-b1a3-9ebc69e96a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tft5_for_conditional_generation_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " shared (Embedding)          multiple                  16449536  \n",
      "                                                                 \n",
      " encoder (TFT5MainLayer)     multiple                  35330816  \n",
      "                                                                 \n",
      " decoder (TFT5MainLayer)     multiple                  41625344  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 60,506,624\n",
      "Trainable params: 60,506,624\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "2a92437c-b5cd-4da0-b27e-a2e39346c7ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "ff772ae0-d06f-4034-a91f-00d6897a9711",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [1:31:06<00:00, 11.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 1.3294719997610709}\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "generation_data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer, model=model, return_tensors=\"tf\", pad_to_multiple_of=128\n",
    ")\n",
    "\n",
    "def generate_with_xla(batch):\n",
    "    return model.generate(\n",
    "        input_ids=batch[\"input_ids\"],\n",
    "        attention_mask=batch[\"attention_mask\"],\n",
    "        max_new_tokens=128,\n",
    "    )\n",
    "    \n",
    "def compute_metrics():\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for batch, labels in tqdm(tf_validation_set):\n",
    "        predictions = generate_with_xla(batch)\n",
    "        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "        labels = labels.numpy()\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "        decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "        decoded_labels = [[label.strip()] for label in decoded_labels]\n",
    "        all_preds.extend(decoded_preds)\n",
    "        all_labels.extend(decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=all_preds, references=all_labels)\n",
    "    return {\"bleu\": result[\"score\"]}\n",
    "\n",
    "print(compute_metrics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "e8053deb-b846-4c19-85c1-3d1b3d940f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "4206/4206 [==============================] - 836s 199ms/step - loss: 3.0594 - val_loss: 2.5822\n",
      "Epoch 2/3\n",
      "4206/4206 [==============================] - 842s 200ms/step - loss: 2.7819 - val_loss: 2.3919\n",
      "Epoch 3/3\n",
      "4206/4206 [==============================] - 851s 202ms/step - loss: 2.6169 - val_loss: 2.2670\n"
     ]
    }
   ],
   "source": [
    "model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=1)\n",
    "model.save_pretrained(\"tf_model/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72efe43-68f9-4647-a85d-288fbf725237",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 395/468 [1:18:16<14:12, 11.68s/it]"
     ]
    }
   ],
   "source": [
    "print(compute_metrics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628aaf80-ad75-4ccb-ae57-f3cc248066da",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"translate English to Spanish: Legumes share resources with nitrogen-fixing bacteria.\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = TFAutoModelForSeq2SeqLM.from_pretrained(\"tf_model/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b96ec2-2fc7-40ed-8b1e-0b3cd4141dae",
   "metadata": {},
   "source": [
    "## Generate the translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49572d72-508f-4a71-a21e-49e4bab23b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = tokenizer([input_text], return_tensors='np')\n",
    "out = model.generate(**tokenized, max_length=128)\n",
    "print(out)\n",
    "\n",
    "with tokenizer.as_target_tokenizer():\n",
    "    print(tokenizer.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbf168e-b2b3-4a3d-ace9-3cb59ff95d70",
   "metadata": {},
   "source": [
    "# Second Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8f9df47a-7428-4209-abfd-82de58cca01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fb8aa3-9f4a-4c86-8d62-110c7bc10f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"This is a sentence that we want to translate.\"\n",
    "\n",
    "tokenizer.src_lang = \"en_XX\"\n",
    "tokenizer.tgt_lang = \"es_XX\"\n",
    "encoded_input = tokenizer(input_text, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28553470-24a1-481b-8795-17f08abcffde",
   "metadata": {},
   "source": [
    "## Translate the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c763253a-842f-4012-96a1-16d8ce2ac324",
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_tokens = model.generate(**encoded_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57a7ed9-981b-4b70-8977-292911d4f822",
   "metadata": {},
   "source": [
    "## Tokens to strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d327fda8-5dbd-4919-b6bd-9534c431d348",
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_text = tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n",
    "print(translated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781963d3-1e57-43ef-9571-fe73a927a9b3",
   "metadata": {},
   "source": [
    "# Third implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51661bf3-7575-4c57-8b3c-66e418229e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "model_name = \"Helsinki-NLP/opus-mt-en-es\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "# Example usage\n",
    "input_text = \"Hello, how are you?\"\n",
    "tokenized = tokenizer([input_text], return_tensors='pt', max_length=512, truncation=True, padding=\"max_length\")\n",
    "translated = model.generate(**tokenized)\n",
    "translated_text = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "print(translated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
