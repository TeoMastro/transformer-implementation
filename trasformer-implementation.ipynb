{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "000fc1af-8dc3-407a-8fc6-6df3e2d2bbbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (4.36.2)\n",
      "Requirement already satisfied: datasets in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (2.16.1)\n",
      "Requirement already satisfied: evaluate in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (0.4.1)\n",
      "Requirement already satisfied: sacrebleu in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (2.4.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from transformers) (0.20.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from transformers) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from transformers) (0.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from datasets) (14.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from datasets) (2.1.4)\n",
      "Requirement already satisfied: xxhash in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from datasets) (3.9.1)\n",
      "Requirement already satisfied: responses<0.19 in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: portalocker in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from sacrebleu) (2.8.2)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from sacrebleu) (0.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from sacrebleu) (0.4.6)\n",
      "Requirement already satisfied: lxml in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from sacrebleu) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from requests->transformers) (2023.11.17)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from pandas->datasets) (2023.4)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from portalocker->sacrebleu) (305.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\teo\\anaconda3\\envs\\tensorgpu\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers datasets evaluate sacrebleu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637acde7-e550-40c2-81f4-b59c229a05e6",
   "metadata": {},
   "source": [
    "# Load a sample dataset and split to train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fb2cb0e-82b3-4d99-9c98-289b27bf1b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "books = load_dataset(\"opus_books\", \"en-es\") # isws vrw kai allo ena dataset en-es\n",
    "books = books[\"train\"].train_test_split(test_size=0.2)\n",
    "\n",
    "books_train_validation_split = books[\"train\"].train_test_split(test_size=0.1)\n",
    "\n",
    "books[\"train\"] = books_train_validation_split[\"train\"]\n",
    "books[\"validation\"] = books_train_validation_split[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be2ec24-6535-4885-a81b-6806c4ce11c9",
   "metadata": {},
   "source": [
    "# An example of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f25df30-222d-4845-909c-64af46bb493a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '92627',\n",
       " 'translation': {'en': 'I afterwards learned that it was to discover if we had any diamonds concealed. This practice had been established since time immemorial among those civilized nations that scour the seas. I was informed that the religious Knights of Malta never fail to make this search whenever any Moors of either sex fall into their hands. It is a part of the law of nations, from which they never deviate.',\n",
       "  'es': 'mas luego supe que era por ver si en aquel sitio habíamos escondido algunos diamantes, y que es estilo establecido de tiempo inmemorial en las naciones civilizadas que andan barriendo los mares, y que los señores religiosos caballeros de Malta nunca le omiten quando apresan á Turcos ó Turcas, porque es ley del derecho de gentes, que nunca ha sido quebrantada.'}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f539bab-e3a9-402e-a997-9c960dcc5725",
   "metadata": {},
   "source": [
    "# Import a tokenizer to process language pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0534457a-c896-4539-be30-ae19a8303e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, model_max_length=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24d5c25-8843-48e9-af8f-31707fd61f05",
   "metadata": {},
   "source": [
    "# Preprocess language pairs by tokenizing inputs and targets separately\n",
    "(since you can’t tokenize Spanish text with a tokenizer pretrained on an English vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34b3cb78-df06-488e-9e73-7e2479f1cb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_lang = \"en\"\n",
    "target_lang = \"es\"\n",
    "prefix = \"translate English to Spanish: \"\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + example[source_lang] for example in examples[\"translation\"]]\n",
    "    targets = [example[target_lang] for example in examples[\"translation\"]]\n",
    "    model_inputs = tokenizer(inputs, text_target=targets, max_length=128, truncation=True) # 128 for t5-small\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0786645f-64ab-4ca7-9f51-9ed03fe721d3",
   "metadata": {},
   "source": [
    "# Apply the preprocess function to the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6a6e219-3735-411d-ab15-6e92d184368e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5826a9ade57b4028b6bd1828f4346962",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/67298 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "959756678f44407d912e69546e0babbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/18694 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "500d9e1b7b1a42fb8e8a27ec89ea51a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7478 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_books = books.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbbfe40-dc49-4c66-864a-021861e356a9",
   "metadata": {},
   "source": [
    "# Create batches of dict-like objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc9b911a-9a9b-4c7d-8f52-9b1606ead286",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b760de15-af3d-46a3-850e-ef5cc4715b63",
   "metadata": {},
   "source": [
    "# Import a metric for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6908e6cd-2f1e-47fa-ab5c-00b5b65bfe26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "# more metrics\n",
    "metric = evaluate.load(\"sacrebleu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02ada2d-cbe7-40ce-adda-9543623025f1",
   "metadata": {},
   "source": [
    "# Create a function that passes your predictions and labels to compute to calculate the SacreBLEU score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f68a06f-2175-4985-b816-84f60786d84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    try:\n",
    "        preds, labels = eval_preds\n",
    "        if isinstance(preds, tuple):\n",
    "            preds = preds[0]\n",
    "        decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    \n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "        decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "    \n",
    "        result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "        result = {\"bleu\": result[\"score\"]}\n",
    "    \n",
    "        prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "        result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "        result = {k: round(v, 4) for k, v in result.items()}\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(\"Error in compute_metrics:\", e)\n",
    "        # Optionally, re-raise the error after logging\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c24fda8-9c7b-4de7-8c9d-81e71ec0a21e",
   "metadata": {},
   "source": [
    "# Transformer finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea83d75f-49a1-4a6f-87dc-1b8e40213946",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamWeightDecay\n",
    "\n",
    "optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "412864b5-1420-4cc5-991c-94167bdb52b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForSeq2SeqLM\n",
    "\n",
    "model = TFAutoModelForSeq2SeqLM.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5619fb7b-f555-4c6b-a8ae-67167f081838",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "tf_train_set = model.prepare_tf_dataset(\n",
    "    tokenized_books[\"train\"],\n",
    "    shuffle=True,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "tf_test_set = model.prepare_tf_dataset(\n",
    "    tokenized_books[\"test\"],\n",
    "    shuffle=False,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "tf_validation_set = model.prepare_tf_dataset(\n",
    "    tokenized_books[\"validation\"],\n",
    "    shuffle=False,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60058168-f529-4e91-a06c-a3875b86f73b",
   "metadata": {},
   "source": [
    "# Compile and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b88fbf51-34f7-4e14-9c27-ca44e9e03569",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)\n",
    "model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccb9990-eebd-4fcc-8361-07fd71a57c88",
   "metadata": {},
   "source": [
    "## See the model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc2bf6b7-a208-41fb-b1a3-9ebc69e96a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tft5_for_conditional_generation\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " shared (Embedding)          multiple                  16449536  \n",
      "                                                                 \n",
      " encoder (TFT5MainLayer)     multiple                  35330816  \n",
      "                                                                 \n",
      " decoder (TFT5MainLayer)     multiple                  41625344  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 60,506,624\n",
      "Trainable params: 60,506,624\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a92437c-b5cd-4da0-b27e-a2e39346c7ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff772ae0-d06f-4034-a91f-00d6897a9711",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [1:17:36<00:00,  9.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 0.4436166800441257}\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "generation_data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer, model=model, return_tensors=\"tf\", pad_to_multiple_of=128\n",
    ")\n",
    "\n",
    "def generate_with_xla(batch):\n",
    "    return model.generate(\n",
    "        input_ids=batch[\"input_ids\"],\n",
    "        attention_mask=batch[\"attention_mask\"],\n",
    "        max_new_tokens=128,\n",
    "    )\n",
    "    \n",
    "def compute_metrics():\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for batch, labels in tqdm(tf_validation_set):\n",
    "        predictions = generate_with_xla(batch)\n",
    "        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "        labels = labels.numpy()\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "        decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "        decoded_labels = [[label.strip()] for label in decoded_labels]\n",
    "        all_preds.extend(decoded_preds)\n",
    "        all_labels.extend(decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=all_preds, references=all_labels)\n",
    "    return {\"bleu\": result[\"score\"]}\n",
    "\n",
    "print(compute_metrics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e8053deb-b846-4c19-85c1-3d1b3d940f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4206/4206 [==============================] - 877s 205ms/step - loss: 3.2119 - val_loss: 2.6612\n",
      "Epoch 2/10\n",
      "4206/4206 [==============================] - 861s 205ms/step - loss: 2.8394 - val_loss: 2.4391\n",
      "Epoch 3/10\n",
      "4206/4206 [==============================] - 863s 205ms/step - loss: 2.6559 - val_loss: 2.2992\n",
      "Epoch 4/10\n",
      "4206/4206 [==============================] - 864s 205ms/step - loss: 2.5288 - val_loss: 2.2007\n",
      "Epoch 5/10\n",
      "4206/4206 [==============================] - 868s 206ms/step - loss: 2.4299 - val_loss: 2.1249\n",
      "Epoch 6/10\n",
      "4206/4206 [==============================] - 864s 205ms/step - loss: 2.3525 - val_loss: 2.0611\n",
      "Epoch 7/10\n",
      "4206/4206 [==============================] - 863s 205ms/step - loss: 2.2870 - val_loss: 2.0093\n",
      "Epoch 8/10\n",
      "4206/4206 [==============================] - 864s 205ms/step - loss: 2.2304 - val_loss: 1.9638\n",
      "Epoch 9/10\n",
      "4206/4206 [==============================] - 863s 205ms/step - loss: 2.1818 - val_loss: 1.9274\n",
      "Epoch 10/10\n",
      "4206/4206 [==============================] - 863s 205ms/step - loss: 2.1392 - val_loss: 1.8932\n"
     ]
    }
   ],
   "source": [
    "model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=10)\n",
    "model.save_pretrained(\"tf_model/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e72efe43-68f9-4647-a85d-288fbf725237",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [1:33:22<00:00, 11.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 10.883976180425792}\n"
     ]
    }
   ],
   "source": [
    "print(compute_metrics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "628aaf80-ad75-4ccb-ae57-f3cc248066da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the layers of TFT5ForConditionalGeneration were initialized from the model checkpoint at tf_model/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "input_text = \"translate English to Spanish: He wrote a letter to a friend.\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = TFAutoModelForSeq2SeqLM.from_pretrained(\"tf_model/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b96ec2-2fc7-40ed-8b1e-0b3cd4141dae",
   "metadata": {},
   "source": [
    "## Generate the translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "49572d72-508f-4a71-a21e-49e4bab23b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[    0  1289     3    15     7 12563    32     3     9    73     3  3690\n",
      "    839     5     1]], shape=(1, 15), dtype=int32)\n",
      "El escrito a un amigo.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TEO\\anaconda3\\envs\\tensorgpu\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3860: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenized = tokenizer([input_text], return_tensors='np')\n",
    "out = model.generate(**tokenized, max_length=128)\n",
    "print(out)\n",
    "\n",
    "with tokenizer.as_target_tokenizer():\n",
    "    print(tokenizer.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbf168e-b2b3-4a3d-ace9-3cb59ff95d70",
   "metadata": {},
   "source": [
    "# Second Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8f9df47a-7428-4209-abfd-82de58cca01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "43fb8aa3-9f4a-4c86-8d62-110c7bc10f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"Allow raters to post-edit translation and measure difference\"\n",
    "\n",
    "tokenizer.src_lang = \"en_XX\"\n",
    "tokenizer.tgt_lang = \"es_XX\"\n",
    "encoded_input = tokenizer(input_text, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28553470-24a1-481b-8795-17f08abcffde",
   "metadata": {},
   "source": [
    "## Translate the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c763253a-842f-4012-96a1-16d8ce2ac324",
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_tokens = model.generate(**encoded_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57a7ed9-981b-4b70-8977-292911d4f822",
   "metadata": {},
   "source": [
    "## Tokens to strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d327fda8-5dbd-4919-b6bd-9534c431d348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "អនុញ្ញាត ឲ្យ អ្នក វាយតម្លៃ កែសម្រួល ការ បកប្រែ ក្រោយ និង វាស់ ភាព ខុស គ្នា\n"
     ]
    }
   ],
   "source": [
    "translated_text = tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n",
    "print(translated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781963d3-1e57-43ef-9571-fe73a927a9b3",
   "metadata": {},
   "source": [
    "# Third implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "51661bf3-7575-4c57-8b3c-66e418229e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TEO\\anaconda3\\envs\\tensorgpu\\lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:197: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Funciona mal al comparar diferentes tipos de sistemas\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "model_name = \"Helsinki-NLP/opus-mt-en-es\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "# Example usage\n",
    "input_text = \"Works poorly when comparing different kinds of systems\"\n",
    "tokenized = tokenizer([input_text], return_tensors='pt', max_length=512, truncation=True, padding=\"max_length\")\n",
    "translated = model.generate(**tokenized)\n",
    "translated_text = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "print(translated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
